# Week 9,10,11

### 1.**Setup môi trường cài đặt cụm HDFS, YARN**

Tóm tắt : thực hành cài đặt Hadoop cluster trên 3 node , sử dụng Docker để giả lập 

Các cài đặt cần có : - Docker Engine

​                                   - OpenJDK (17.0.9)

​                                   - Java_HOME path

​                                   - Trình soạn thảo Terminal: Vim hoặc Nano

- ##### Cài đặt môi trường 

  -  Tạo một bridge network mới trên Docker

    ```
    $ docker network create hadoop-network
    ```

  - Tạo một container trên image Ubuntu 20.04

    ```
    $ docker run -it --name node1 -p 9870:9870 -p 8088:8088 -p 19888:19888 --hostname node1 --network hadoop-network ubuntu:20.04
    ```

  - Cài đặt các package cần thiết 

    ```
    $ apt update
    $ apt install -y wget tar ssh default-jdk 
    ```

  - Tạo user hadoop 

    ```
    $ groupadd hadoop
    $ useradd -g hadoop -m -s /bin/bash hdfs
    $ useradd -g hadoop -m -s /bin/bash yarn
    $ useradd -g hadoop -m -s /bin/bash mapre
    ```

  - Tạo ssh-key trên mỗi user

    ```
    $ su hdfs
    $ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
    $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
    $ chmod 0600 ~/.ssh/authorized_keys
    ```

    Tương tự với yarn và mapre

  - Start ssh service

    ```
    $ service ssh start
    ```

  - Thêm hostname trong file `/etc/hosts` 

    ```
    172.19.0.2      node1
    ```

  - Kiểm tra xem đã ssh được vào hay chưa

    ```
    $ ssh hdfs@node01
    ```

- **Download Hadoop**

  - Download hadoop (version 3.3.6)

    ```
    wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
    tar -xvzf hadoop-3.3.6.tar.gz
    mv hadoop-3.3.6 /lib/hadoop
    mkdir /lib/hadoop/logs
    chgrp hadoop -R /lib/hadoop
    ```

    

  - Cấu hình biến môi trường, thêm các biến môi trường vào file `/etc/bash.bashrc` để tất cả các user trên hệ thống đều có thể sử dụng

    ```
    export JAVA_HOME=/usr/lib/jvm/default-java
    export HADOOP_HOME=/lib/hadoop
    export PATH=$PATH:$HADOOP_HOME/bin
    
    export HDFS_NAMENODE_USER="hdfs"
    export HDFS_DATANODE_USER="hdfs"
    export HDFS_SECONDARYNAMENODE_USER="hdfs"
    export YARN_RESOURCEMANAGER_USER="yarn"
    export YARN_NODEMANAGER_USER="yarn"
    
    export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
    ```

  - Cập nhật biến môi trường

    ```
    $ source /etc/bash.bashrc
    ```

  - Cập nhật biến môi trường trong file: `$HADOOP_HOME/etc/hadoop/hadoop-env.sh`

    ```
    export JAVA_HOME=/usr/lib/jvm/default-java
    ```

  - Thiết lập cấu hình cho Hadoop

    - `$HADOOP_HOME/etc/hadoop/core-site.xml`

      ```
      <configuration>
          <property>
              <name>fs.defaultFS</name>
              <value>hdfs://node01:9000</value>
          </property>
          <property>
              <name>hadoop.tmp.dir</name>
              <value>/home/${user.name}/hadoop</value>
          </property>
      </configuration>
      ```

    - `$HADOOP_HOME/etc/hadoop/hdfs-site.xml`

      ```
      <configuration>
          <property>
              <name>dfs.replication</name>
              <value>1</value>
          </property>
          <property>
              <name>dfs.permissions.superusergroup</name>
              <value>hadoop</value>
          </property>
          <property>
              <name>dfs.datanode.data.dir.perm</name>
              <value>774</value>
          </property>
      </configuration>
      ```

    - `$HADOOP_HOME/etc/hadoop/yarn-site.xml`

      ```
      <configuration>
          <property>
              <name>yarn.resourcemanager.hostname</name>
              <value>node01</value>
          </property>
      </configuration>
      ```

- ##### Chạy trên 1 node

  - Format file trên Name Node

    ```
    $ su hdfs
    $ $HADOOP_HOME/bin/hdfs namenode -format
    $ exit
    ```

  - Chạy các dịch vụ của Hadoop trên account root

    ```
    $ $HADOOP_HOME/sbin/start-all.sh
    ```

    ![img](https://i.imgur.com/KSByyn6.png)

- ##### Thêm mới 1 node

  - Để thêm một node mới vào cụm thì trên node đó cũng thực hiện đầy đủ các bước ở trên. Do sử dụng Docker nên sẽ tạo một image từ container  đang có

    ```
    $ docker commit node1 hadoop
    ```

  - Run container mới từ image vừa tạo

    ```
    $ docker run -it --name node2 --hostname node2 --network hadoop-network hadoop
    ```

  - Trên node02 ta start service ssh và xoá thư mục data cũ đi

    ```
    $ service ssh start
    $ rm -rf /home/hdfs/hadoop
    $ rm -rf /home/yarn/hadoop
    ```

  - Cập nhật ip, hostname của Namenode cho node02

    - File `/etc/hosts`

      ```
      172.19.0.2      node1
      172.19.0.3      node2
      ```

    - File `/etc/hosts`

      ```
      172.19.0.2      node1
      172.19.0.3      node2
      ```

    - File `$HADOOP_HOME/etc/hadoop/workers`

      ```
      node1
      node2
      ```

  - Start all các dịch vụ của hadoop trên node01

    ```
    $ $HADOOP_HOME/sbin/start-all.sh
    ```

Thực hiện tương tự đối với node3 , ta sẽ có được cụm Hadoop 3 nút :

![img](https://i.imgur.com/qXqkIjt.png)

![img](https://i.imgur.com/BsxePnM.png)

![img](https://i.imgur.com/T5KqaVN.png)



### 2.Chạy chương trình Word Count với Hadoop Mapreduce

Sử dụng docker container đã cài ở phần trước để cài đặt chương trình WordCount

- Chạy HDFS

  ```
  root@node01:~# $HADOOP_HOME/sbin/start-dfs.sh
  ```

- Thêm user root vào group hadoop 

  ```
  root@node1:/# adduser root hadoop
  ```

- Tạo 1 file trên HDFS làm input cho bài toán Wordcount

  ```
  root@node01:~# hdfs dfs -D dfs.replication=2 -appendToFile /lib/hadoop/logs/*.log /input_wordcount.log
  ```

- Tạo thêm thư mục wordcount trong root và thêm source code vào file WordCount.java

  ```
  root@node1:/# mkdir wordcount
  root@node1:/# cd wordcount   
  root@node1:/wordcount# vi WordCount.java
  ```

  ```java
  import java.io.IOException;
  import java.util.StringTokenizer;
  
  import org.apache.hadoop.conf.Configuration;
  import org.apache.hadoop.fs.Path;
  import org.apache.hadoop.io.IntWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Job;
  import org.apache.hadoop.mapreduce.Mapper;
  import org.apache.hadoop.mapreduce.Reducer;
  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
  
  public class WordCount {
  
    public static class TokenizerMapper
         extends Mapper<Object, Text, Text, IntWritable>{
  
      private final static IntWritable one = new IntWritable(1);
      private Text word = new Text();
  
      public void map(Object key, Text value, Context context
                      ) throws IOException, InterruptedException {
        StringTokenizer itr = new StringTokenizer(value.toString());
        while (itr.hasMoreTokens()) {
          word.set(itr.nextToken());
          context.write(word, one);
        }
      }
    }
  
    public static class IntSumReducer
         extends Reducer<Text,IntWritable,Text,IntWritable> {
      private IntWritable result = new IntWritable();
  
      public void reduce(Text key, Iterable<IntWritable> values,
                         Context context
                         ) throws IOException, InterruptedException {
        int sum = 0;
        for (IntWritable val : values) {
          sum += val.get();
        }
        result.set(sum);
        context.write(key, result);
      }
    }
  
    public static void main(String[] args) throws Exception {
      Configuration conf = new Configuration();
      Job job = Job.getInstance(conf, "word count");
      job.setJarByClass(WordCount.class);
      job.setMapperClass(TokenizerMapper.class);
      job.setCombinerClass(IntSumReducer.class);
      job.setReducerClass(IntSumReducer.class);
      job.setOutputKeyClass(Text.class);
      job.setOutputValueClass(IntWritable.class);
      FileInputFormat.addInputPath(job, new Path(args[0]));
      FileOutputFormat.setOutputPath(job, new Path(args[1]));
      System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
  }
  ```

- Bổ sung biến môi trường

  ```
  export HADOOP_CLASSPATH=${JAVA_HOME}/lib/tools.jar
  ```

- Compile code

  ```
  root@node01:~/wordcount# hadoop com.sun.tools.javac.Main WordCount.java
  root@node01:~/wordcount# jar cf wc.jar WordCount*.class
  ```

- Chạy và kiểm tra kết quả

```
root@node01:~/wordcount# hadoop jar wc.jar WordCount /input_wordcount.log /ouput_wordcount 
root@node01:~/wordcount# hdfs dfs -cat /ouput_wordcount/part-r-00000
```

![img](https://i.imgur.com/lX4KJp6.png)

![img](https://i.imgur.com/ZwC8Ax1.png)

![img](https://i.imgur.com/lPTDoxQ.png)

#### - Test thêm cái khác :v

**Tạo input files** 

```
mkdir input
echo "Hello Docker" >input/file2.txt
echo "Hello Hadoop" >input/file1.txt
```

**Tạo thư mục input trên HDFS**

```
hadoop fs -mkdir -p input
```

**Đẩy lên HDFS**

```
hdfs dfs -put ./input/* input
```

**Chạy wordcount** 

```
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-3.3.6-sources.jar
org.apache.hadoop.examples.WordCount input output
```

**In ra input files**

```
echo -e "\ninput file1.txt:"
hdfs dfs -cat input/file1.txt
```

```
echo -e "\ninput file2.txt:"
hdfs dfs -cat input/file2.txt
```

**In ra output  wordcount**

```
echo -e "\nwordcount output:"
hdfs dfs -cat output/part-r-00000
```

![img](https://i.imgur.com/90IxBjW.png)

![image-20240105064816374](/home/son/snap/typora/86/.config/Typora/typora-user-images/image-20240105064816374.png)

![img](https://i.imgur.com/uLkU8s3.png)

### 3.**Cài đặt Spark standalone, spark trên yarn**

- Tải về phiên bản mới nhất của Spark

  ```
  wget https://dlcdn.apache.org/spark/spark-3.3.4/spark-3.3.4-bin-hadoop3.tgz
  ```

- Giải nén file

  ```
  tar -xzvf spark-3.3.4-bin-hadoop3.tgz  
  ```

- Copy thư mục giải nén thành spark

  ```
  mv spark-3.3.4-bin-hadoop3 spark
  chgrp hadoop -R /spark
  chmod g+w -R /spark
  ```

- Bổ sung biến môi trường 

  ```
  root@node1:/# echo 'export SPARK_HOME=/spark' >> ~/.bashrc
  root@node1:/# echo 'export PATH=$PATH:$SPARK_HOME/bin' >> ~/.bashrc
  ```

- Load lại biến môi trường

  ```
  source ~/.bashrc
  ```

- Kiểm tra Spark đã cài thành công hay chưa bằng `spark-shell`

![img](https://i.imgur.com/2xci5w7.png)

- Khởi động Spark Standalone

  ```
  ./start-master.sh
  ```

- Mở trình duyệt web và nhập địa chỉ IP localhost trên cổng 8080 để xem giao diện người dùng Spark Web

  ![img](https://i.imgur.com/9GpZIHb.png)

- Spark chạy trên Yarn 

  ![img](https://i.imgur.com/Ssz6oZn.png)

  ![image-20240103171840063](/home/son/snap/typora/86/.config/Typora/typora-user-images/image-20240103171840063.png)

  ![img](https://i.imgur.com/8amGqSy.png)

#### 3.1.Chạy được chương trình word count với spark chạy trên yarn

- Tạo file input đầu vào cho chương trình 

  ```
  root@node1:/# vi SparkWC.txt
  ```

- Tải tệp sparkWC.txt lên HDFS trong thư mục cụ thể.

  ```
  root@node1:/# hdfs dfs -copyFromLocal SparkWC.txt /user/root/input_spark
  ```

- Submit nó lên cluster YARN

  ```
  root@node1:/# spark-shell --master yarn --deploy-mode client
  ```

  ![img](https://i.imgur.com/zLobxVv.png)

  ![img](https://i.imgur.com/f6q4BKP.png)

- Chạy WordCount

  ```
  #Đọc dữ liệu từ tệp văn bản trên HDFS và tạo một RDD
  scala> val data = sc.textFile("hdfs://node1:9000/user/root/input_spark")
  scala> data.collect()
  
  #phân tách mỗi dòng thành các từ riêng lẻ
  scala> val splitdata = data.flatMap(line => line.split(" "));  
  scala> splitdata.collect;  
  
  #Chuyển đổi mỗi từ thành một cặp key-value
  scala> val mapdata = splitdata.map(word => (word,1));  
  scala> mapdata.collect;  
  
  #Cộng tổng value theo key
  scala> val reducedata = mapdata.reduceByKey(_+_);  
  scala> reducedata.collect; 
  
  #Đặt đường dẫn đến thư mục trên HDFS để lưu kết quả WordCount.
  scala> val outputPath = "hdfs://node1:9000/user/root/output_spark"
  
  #Lưu RDD reducedata (kết quả WordCount) vào thư mục trên HDFS được chỉ định
  scala> reducedata.saveAsTextFile(outputPath)
  ```

  ![img](https://i.imgur.com/z8npHkh.png)

![img](https://i.imgur.com/EQSSgCQ.png)

![img](https://i.imgur.com/zzTccJR.png)

#### 3.2.Tự sinh dữ liệu người dùng , visualize lên bằng pyspark.

- Cài gói “PySpark” để sử dụng Spark với ngôn ngữ Python.

  ```
  apt install python3-pip
  
  pip install pyspark
  ```

- Kiểm tra lại Python đã kết nối được Spark chưa

  ![img](https://i.imgur.com/eqVsieJ.png)

##### 1.Sinh file parquet trên hệ thống hdfs đã cài đặt: khoảng 1 triệu bản ghi gồm các cột: tên, ngày sinh, địa chỉ (địa chỉ random từ 1-100), giới tính, số điện thoại. 

- **Tạo một thư mục để chứa script Python và chạy script:**

```
mkdir generate_data
cd generate_data
```

- Thêm đoạn mã python vào file sau :

```
vi generate_data.py
```

- Chạy spark submit

  ```
  root@node1:/generate_data# spark-submit generate_data.py
  ```

![img](https://i.imgur.com/avgrFCW.png)

- Kiểm tra trên HDFS 

  ![img](https://i.imgur.com/6SCYCn7.png)

##### 2.Chạy jupyter notebook 

- Cài đặt Jupyter

```
pip install jupyter
```

- Chạy jupyter notebook 

```bash
jupyter notebook --ip 0.0.0.0 --port 8888 --no-browser --allow-root
```

![img](https://i.imgur.com/9bFiihv.png)

##### 3.Sử dụng pyspark để đọc file trên notebook.

- Cài đặt thư viện `findspark`:

```
pip install findspark
```

- Chạy PySpark để đọc file Parquet

  ```
  pyspark
  ```

  ```
  from pyspark.sql import SparkSession
  # Tạo Spark Session
  spark = SparkSession.builder.appName("ReadUserData").getOrCreate()
  # Đọc file Parquet từ HDFS
  df = spark.read.parquet("hdfs://node1:9000/user/root/parquet_data_faker")
  # Hiển thị một số dòng dữ liệu
  df.show(5)
  ```

![img](https://i.imgur.com/1f5uX6s.png)



##### 4.Visualize: thống kê lượng user theo tuổi (khoảng độ tuổi ví dụ từ 10-20, 20-30 tuổi, ...), giới tính bằng các biểu đồ hợp lý

- Cài đặt các thư viện hỗ trợ viasualize như Numpy,Pandas và Matplotlib 

- Code Python 

  ```
  from pyspark.sql import SparkSession
  from pyspark.sql import functions as F
  import pandas as pd
  import matplotlib.pyplot as plt
  
  # Khởi tạo Spark session
  spark = SparkSession.builder.appName("read_parquet").getOrCreate()
  
  # Đọc dữ liệu từ file parquet
  df = spark.read.parquet("hdfs://node1:9000/user/root/parquet_data_faker")
  
  # Tính toán cột age từ birthdate
  df = df.withColumn("age", F.floor(F.datediff(F.current_date(), "birthdate") / 365.25))
  
  # Thống kê số lượng theo giới tính và khoảng tuổi
  result = df.groupBy("gender", F.expr("floor(age/10)*10").alias("age_group")).count()
  
  # Collect dữ liệu về máy local
  result_local = result.collect()
  
  # Chuyển đổi thành Pandas DataFrame
  result_pd = pd.DataFrame(result_local, columns=result.columns)
  
  # Vẽ biểu đồ
  result_pd.pivot(index='age_group', columns='gender', values='count').plot(kind='bar', stacked=True)
  
  plt.title('User and gender')
  plt.xlabel('Age')
  plt.ylabel('Count')
  plt.savefig('bar_chart.png')
  
  ```

- Chạy file code , file ảnh sẽ được tạo ra trong thư mục, kiểm tra trên Jupiter Notebook 

  ![img](https://i.imgur.com/8p6BGic.png)

![img](https://i.imgur.com/mJKCsfE.png)

- Ngoài ra ta có thể chạy code trên và visualize trên chính Jupiter Lab,tạo file python3 mới,thêm code vào một Cell mới và chạy :

![img](https://i.imgur.com/TdgSoYd.png)



##### 5.Chuyển các code pyspark về code java và submit với spark chạy trên yarn. 

- Chuyển code mục **3.** về Java

  ```
  import org.apache.spark.sql.SparkSession;
  import org.apache.spark.sql.Dataset;
  import org.apache.spark.sql.Row;
  import org.apache.spark.sql.functions;
  import org.apache.spark.sql.expressions.Window;
  import org.apache.spark.sql.expressions.WindowSpec;
  import static org.apache.spark.sql.functions.*;
  
  public class ReadParquet {
      public static void main(String[] args) {
          SparkSession spark = SparkSession.builder().appName("ReadParquet").getOrCreate();
  
          Dataset<Row> df = spark.read().parquet("hdfs://node1:9000/user/root/parquet_data_faker");
  
          df = df.withColumn("age", floor(datediff(current_date(), col("birthdate")) / 365.25))
                  .groupBy("gender", expr("floor(age/10)*10").alias("age_group"))
                  .count();
  
          df.show(5);
  
          spark.stop();
      }
  }
  
  ```

- Compile và chuyển thành file jar:

  ```
  root@node1:/generate_data# javac -cp "/lib/spark/jars/*" ReadParquet.java
  root@node1:/generate_data# jar cf ReadParquet.jar ReadParquet*.class
  ```

![img](https://i.imgur.com/eM9cKHm.png)

- Chuyển code mục **4.** về java (chỉ cần hiển thị số liệu)

  ```
  import org.apache.spark.sql.Dataset;
  import org.apache.spark.sql.Row;
  import org.apache.spark.sql.SparkSession;
  import static org.apache.spark.sql.functions.*;
  
  public class UserStatistics {
  
      public static void main(String[] args) {
          SparkSession spark = SparkSession.builder()
                  .appName("UserStatistics")
                  .getOrCreate();
  
          Dataset<Row> df = spark.read().parquet("hdfs://node1:9000/user/root/parquet_data_faker");
  
          df = df.withColumn("age", floor(datediff(current_date(), col("birthdate")).divide(365.25)).cast("int"));
  
          Dataset<Row> result = df.groupBy(col("gender"), expr("floor(age/10)*10 as age_group")).count();
  
          result.show(7);
      }
  }
  ```

- Compile code và submit tương tự

  ```
  root@node1:/generate_data# javac -cp "/lib/spark/jars/*" UserStatistics.java
  root@node1:/generate_data# jar cf UserStatistics.jar UserStatistics.class
  root@node1:/generate_data# spark-submit --class UserStatistics --master yarn --deploy-mode client UserStatistics.jar
  ```

  ![img](https://i.imgur.com/bgV6LEg.png)
